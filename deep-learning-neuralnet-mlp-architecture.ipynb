{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Assignment 1\n",
    "___________\n",
    "### Group Members\n",
    "#### Danqing Yin - 430563708\n",
    "#### Prerona Majumder - 470531989\n",
    "#### Ryan Clark - 470295634\n",
    "\n",
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section marked 'Add Inputs' is used to set file locations for testing run by assignment marker\n",
    "\n",
    "Please complete teh file locations, 'final_test' locations are made for the unseen data\n",
    "\n",
    "analysis can be set to True if the marker would like to see the built in training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports libraries needed\n",
    "#      -- NOTE: Sklearn is only used outside of the Neural network for measurement\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Add Inputs Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/input/train_128.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9eb12fbcd55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_file_location\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfinal_test_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_label_file_location\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfinal_test_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ryan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ryan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/input/train_128.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#Add the input file locations here\n",
    "\n",
    "training_data_file_location = 'Assignment1_Dataset/train_128.h5'\n",
    "training_label_file_location = 'Assignment1_Dataset/train_label.h5'\n",
    "final_test_file_location = 'Assignment1_Dataset/train_128.h5'\n",
    "final_test_file_location = 'Assignment1_Dataset/train_label.h5'\n",
    "\n",
    "#Set analysis = True to see training analysis\n",
    "analysis = False\n",
    "\n",
    "\n",
    "with h5py.File(training_data_file_location,'r') as H: final_test_data = np.copy(H['data'])\n",
    "with h5py.File(training_label_file_location,'r') as H: final_test_label = np.copy(H['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\" Compute the Softmax function.\n",
    "        \"\"\"\n",
    "        exps = np.exp(z - np.max(z))\n",
    "        return exps / np.sum(exps)\n",
    "        \n",
    "class Tanh:        \n",
    "    def activation(z):\n",
    "        \"\"\" Compute the tanh function or its derivative.\n",
    "        \"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def prime(z):\n",
    "        return 1 - np.square(Tanh.activation(z))\n",
    "    \n",
    "\n",
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\" Compute the Relu function or its derivative.\n",
    "        \"\"\"\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\" Compute the Sigmoid function or its derivative.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "    \n",
    "    \n",
    "class CrossEntropy:\n",
    "    def __init__(self, activation_fn=Softmax):\n",
    "        self.m = None\n",
    "        \"\"\"\n",
    "        :param activation_fn: Class object of the activation function.\n",
    "        \"\"\"\n",
    "        if activation_fn:\n",
    "            self.activation_fn = activation_fn\n",
    "            print(activation_fn)\n",
    "            \n",
    "        else:\n",
    "            self.activation_fn = NoActivation\n",
    "            \n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_onehot, x):\n",
    "        \"\"\" Compute the Cross Entropy loss function or its derivative.\n",
    "        \"\"\"\n",
    "        indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "        #x = Softmax.activation(x)\n",
    "        predicted_probability = x[np.arange(len(x)), indices]\n",
    "        log_preds = np.log(predicted_probability)\n",
    "        log_preds[np.isnan(log_preds)] = 0 \n",
    "        loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "        return loss\n",
    "    \n",
    "    def delta(self, y, a):\n",
    "        \"\"\"\n",
    "        Cp_a, dC/da: the derivative of C w.r.t a\n",
    "        ''a'' is the output of neurons\n",
    "        ''y'' is the expected output of neurons\n",
    "        \"\"\"\n",
    "        return (a - y) # delta\n",
    "        \n",
    "class Momentum():       \n",
    "    \"\"\" Compute the Momentum\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, weights, velocity, eta, dw):\n",
    "        self.w = weights\n",
    "        self.v = velocity\n",
    "        self.eta = eta\n",
    "        self.dw = dw\n",
    "        \n",
    "    def update(self):\n",
    "        \n",
    "        gamma = 0.1\n",
    "        \n",
    "        self.v = gamma*self.v + self.eta*self.dw\n",
    "        self.w -= self.v\n",
    "        \n",
    "        return self.w, self.v        \n",
    "        \n",
    "class NoActivation:\n",
    "    \"\"\"\n",
    "    This is a plugin function for no activation.\n",
    "    f(x) = x * 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\"\n",
    "        :param z: (array) w(x) + b\n",
    "        :return: z (array)\n",
    "        \"\"\"\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        \"\"\"\n",
    "        The prime of z * 1 = 1\n",
    "        :param z: (array)\n",
    "        :return: z': (array)\n",
    "        \"\"\"\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimensions, activations):\n",
    "        \"\"\"\n",
    "        Creates the layers a actiations of the network\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "\n",
    "  \n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "        # velocity\n",
    "        self.vel = {}\n",
    "\n",
    "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.vel[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])  \n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "            \n",
    "    def compute_dropout(self, activations, dropout_prob):\n",
    "        \"\"\"Sets a proportion of the activations to zero\n",
    "        \"\"\"\n",
    "            \n",
    "        activations/=dropout_prob    \n",
    "        mult = np.random.binomial(1, 1- dropout_prob, size = activations.shape)\n",
    "        activations*=mult\n",
    "        return activations\n",
    "    \n",
    "    def _feed_forward(self, x, do_dropout, dropout_prob):\n",
    "        \"\"\"\n",
    "        Execute a forward feed through the network.\n",
    "        :param x: (array) Batch of input data vectors.\n",
    "        :return: (tpl) Node outputs and activations per layer.\n",
    "        \"\"\"\n",
    "        # w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # activations: f(z)\n",
    "        a = {1: x}  # First layer has no activations as input. The input x is the input.\n",
    "\n",
    "        for i in range(1, self.n_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.w[i]) + self.b[i]\n",
    "            if i < (self.n_layers - 1):\n",
    "                if do_dropout:\n",
    "                    a[i + 1] = self.compute_dropout(self.activations[i + 1].activation(z[i + 1]), self.dropout_prob)\n",
    "                else:\n",
    "                    a[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "            if i == (self.n_layers - 1):\n",
    "         \n",
    "                a[i + 1] = z[i + 1]\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _back_prop(self, z, a, y_true, momentum, l2):\n",
    "        \"\"\"\n",
    "        The input dicts keys represent the layers of the net.\n",
    "        a = { 1: x,\n",
    "              2: f(w1(x) + b1)\n",
    "              3: f(w2(a2) + b2)}\n",
    "        :param z: (dict) w(x) + b\n",
    "        :param a: (dict) f(z)\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine partial derivative and delta for the output layer.\n",
    "        # delta output layer\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "        \n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # In case of three layer net (two hidden layers) will iterate over i = 3 and i = 2\n",
    "        # Determine partial derivative and delta for the rest of the layers.\n",
    "        # Each iteration requires the delta from the previous layer, propagating backwards.\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.w[i].T) * self.activations[i].prime(z[i])\n",
    "            dw = np.dot(a[i - 1].T, delta) + (self.l2/len(y_true))*self.w[i-1]\n",
    "            L2_term = ((self.l2/(2*y_true.shape[0])) * (np.sum(self.w[i-1] ** 2.) + np.sum(self.w[i] ** 2.)))\n",
    "            delta += L2_term\n",
    "            update_params[i - 1] = (dw, delta)\n",
    "        \n",
    "        for k, v in update_params.items():\n",
    "            self._update_w_b(k, v[0], v[1], momentum)\n",
    "\n",
    "    def _update_w_b(self, index, dw, delta, momentum):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        :param index: (int) Number of the layer\n",
    "        :param dw: (array) Partial derivatives\n",
    "        :param delta: (array) Delta error.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w[index] -= self.learning_rate * dw\n",
    "        self.b[index] -= self.learning_rate * np.mean(delta, 0)\n",
    "        \n",
    "        # momentum\n",
    "        if momentum:  \n",
    "            self.w[index], self.vel[index] = Momentum(self.w[index], self.vel[index], self.learning_rate, dw).update()\n",
    "        else:\n",
    "            self.w[index] -= self.learning_rate * dw\n",
    "        \n",
    "\n",
    "    def fit(self, x, y_true, loss, epochs, batch_size, learning_rate, learning_rate_decay, momentum, do_dropout, dropout_prob, l2):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :param y_true: (array) Containing one hot encoded labels.\n",
    "        :param loss: Loss class (MSE, CrossEntropy etc.)\n",
    "        :param epochs: (int) Number of epochs/iterations.\n",
    "        :param batch_size: (int) Number of samples in minibatch\n",
    "        :param learning_rate: (flt)\n",
    "        :param momentum\n",
    "        \"\"\"\n",
    "        \n",
    "        if not x.shape[0] == y_true.shape[0]:\n",
    "            raise ValueError(\"Length of x and y arrays don't match\")\n",
    "        # Initiate the loss object with the final activation function\n",
    "        self.l2 = l2\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.do_dropout = do_dropout\n",
    "        self.dropout_prob = dropout_prob\n",
    "        loss_list = []\n",
    "        acc_list = []\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "            # minibatch\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self._feed_forward(x_[k:l], self.do_dropout, self.dropout_prob)\n",
    "                self._back_prop(z, a, y_[k:l], momentum, l2)\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                _, a = self._feed_forward(x, self.do_dropout, self.dropout_prob)\n",
    "                loss = self.loss.loss(y_true, a[self.n_layers])\n",
    "                loss_list = np.append(loss_list, loss)\n",
    "                print(\"Loss in %s epochs: %f\" % (i + 1, loss))\n",
    "                \n",
    "                t_results = nn.predict(x_test,False, 0)\n",
    "                correct = 0\n",
    "                for i in range(len(y_test)):\n",
    "                    pred = np.argmax(t_results[i])\n",
    "                    true = y_test[i]\n",
    "                    if pred == true :\n",
    "                        correct += 1\n",
    "                print('Accuracy = '+ str(correct/len(y_test)*100) \\\n",
    "                      + ' of ' + str(len(y_test)) \\\n",
    "                      + ' Samples. ')\n",
    "                acc_list = np.append(acc_list,correct/len(y_test))\n",
    "            # Learning rate decay each epochs to avoid local minima  \n",
    "            self.learning_rate = self.learning_rate * (self.learning_rate / (self.learning_rate + (self.learning_rate * self.learning_rate_decay)))\n",
    "\n",
    "        return i, loss_list, acc_list\n",
    "    \n",
    "    \n",
    "    def predict(self, x, do_dropout, dropout_prob):\n",
    "        self.do_dropout = do_dropout\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :return: (array) A 2D array of shape (n_cases, n_classes).\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(x, self.do_dropout, self.dropout_prob)\n",
    "        return a[self.n_layers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Softmax'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:70: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in 10 epochs: 1.067303\n",
      "Accuracy = 73.49 of 10000 Samples. \n",
      "Loss in 20 epochs: 0.432268\n",
      "Accuracy = 82.47 of 10000 Samples. \n"
     ]
    }
   ],
   "source": [
    "#This Section runs the training model\n",
    "if __name__ == \"__main__\":\n",
    "    with h5py.File(training_data_file_location,'r') as H: data = np.copy(H['data'])\n",
    "    with h5py.File(training_label_file_location,'r') as H: label = np.copy(H['label'])\n",
    "\n",
    "    #Set Validation set\n",
    "    start = 40000  #pic first record from data for validation\n",
    "    end = 50000 ##pic last record from data for validation\n",
    "    x_test = data[start:end]\n",
    "    y_test = label[start:end]\n",
    " \n",
    "    #Unused data normalisationo\n",
    "    #data = (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    x = data[:40000] #Creation of training set (records have)\n",
    "    y = label[:40000]\n",
    "\n",
    "    y = np.eye(10)[y]\n",
    "\n",
    "    \n",
    "    nn = Network((128, 96, 54, 10), (Relu, Tanh, Softmax))\n",
    "    e,losses,acc = nn.fit(x, y, loss=CrossEntropy, epochs=500, \\\n",
    "           batch_size=50, learning_rate=1e-4, learning_rate_decay=0.0001, \\\n",
    "           momentum = True, do_dropout = True, dropout_prob = 0.5, l2 =0.0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In Sample Testing\n",
    "\n",
    "if analysis:\n",
    "    prediction = nn.predict(x, do_dropout = False, dropout_prob = 0.5)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(y)):\n",
    "        y_pred.append(np.argmax(prediction[i]))\n",
    "        y_true.append(np.argmax(y[i]))\n",
    "\n",
    "    print(metrics.classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Testing without dropout\n",
    "if analysis:\n",
    "    prediction = nn.predict(x_test, do_dropout = False, dropout_prob = 0)\n",
    "    #print(prediction)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(y_test)):\n",
    "        y_pred.append(np.argmax(prediction[i]))\n",
    "        y_true.append((y_test[i]))\n",
    "\n",
    "    print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run prediction over provided held out Testing Set for accuracy\n",
    "if analysis:\n",
    "    start_ = 50000  #pic first record from testing set \n",
    "    end_ = 60000 ##pic last record from testing set\n",
    "    x_test_ho = data[start_:end_]\n",
    "    y_test_ho = label[start_:end_]\n",
    "\n",
    "    t_results = nn.predict(x_test_ho, do_dropout = False, dropout_prob = 0.)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(y_test_ho)):\n",
    "        pred = np.argmax(t_results[i])\n",
    "        true = y_test_ho[i]\n",
    "        if pred == true :\n",
    "            correct += 1\n",
    "    print('Accuracy = '+ str(correct/len(y_test_ho)*100) + ' of ' + str(len(y_test_ho)) + ' Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "if analysis:\n",
    "    e = np.arange(1,len(losses)+1)\n",
    "    pl.figure(figsize=(15,4))\n",
    "    pl.plot(e*10, losses)\n",
    "    pl.plot(e*10, acc)\n",
    "    pl.grid()\n",
    "    pl.grid(color='w', linestyle='-', linewidth=1)\n",
    "    pl.xlabel('Epochs')\n",
    "    pl.ylabel('Loss & Accuracy')\n",
    "\n",
    "\n",
    "    pl.suptitle('Loss in training and Accuracy in Validation over Epochs', fontsize=16)\n",
    "\n",
    "\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 82.515 of 60000 Samples\n"
     ]
    }
   ],
   "source": [
    "#Testing of Real With held data for Marking\n",
    "x_test_final = final_test_data\n",
    "y_test_final = final_test_label\n",
    "\n",
    "results = nn.predict(x_test_final, do_dropout = False, dropout_prob = 0.)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(y_test_final)):\n",
    "    pred = np.argmax(results[i])\n",
    "    true = y_test_final[i]\n",
    "    if pred == true :\n",
    "        correct += 1\n",
    "print('Accuracy = '+ str(correct/len(y_test_final)*100) + ' of ' + str(len(y_test_final)) + ' Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.81      0.79      6000\n",
      "          1       1.00      0.93      0.96      6000\n",
      "          2       0.74      0.69      0.71      6000\n",
      "          3       0.81      0.88      0.84      6000\n",
      "          4       0.66      0.82      0.73      6000\n",
      "          5       0.93      0.89      0.91      6000\n",
      "          6       0.65      0.49      0.55      6000\n",
      "          7       0.89      0.88      0.88      6000\n",
      "          8       0.93      0.94      0.93      6000\n",
      "          9       0.90      0.93      0.91      6000\n",
      "\n",
      "avg / total       0.83      0.83      0.82     60000\n",
      "\n",
      "Final Model Accuracy = 82.515 of 60000 Samples\n"
     ]
    }
   ],
   "source": [
    "#Testing of Real With held data for Marking\n",
    "prediction = nn.predict(x_test_final, do_dropout = False, dropout_prob = 0)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(y_test_final)):\n",
    "    y_pred.append(np.argmax(prediction[i]))\n",
    "    y_true.append((y_test_final[i]))\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "print('Final Model Accuracy = '+ str(correct/len(y_test_final)*100) + ' of ' + str(len(y_test_final)) + ' Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
